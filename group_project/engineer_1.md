Report #1
Github: https://github.com/mahlernim/warfarin_prediction_kscpt_tutorial
Outcome: Partial success.
Process: I cloned the repo and tried to run the main Jupyter notebook (KSCPT workshop.ipynb). The notebook uses a cleaned IWPC dataset (IWPC_cleaned.csv) that’s already included, which is nice because I didn’t have to do the full data-cleaning pipeline myself. I was able to get the earlier parts working (loading the data and running the basic model training steps). But one later section broke because the code calls a scikit-learn function that doesn’t exist anymore in newer versions. Specifically, the notebook uses sklearn.inspection.plot_partial_dependence to make partial dependence plots. That function was deprecated and then removed in newer scikit-learn releases, so my run failed at that point with an error about the function being missing. To fix it, I would either need to install an older scikit-learn version that still has that function (the paper reports using scikit-learn 1.0.1), or update the code to use the modern replacement (PartialDependenceDisplay.from_estimator). Because I couldn’t complete the partial dependence plot section as written, I wasn’t able to fully run the whole tutorial end-to-end in my current environment.
Engineer: Haider Gilani 


Report #2
Github: https://github.com/cpockrandt/genmap
Outcome: Partial success.
Process: I cloned the genmap repo and followed the README. GenMap is a command-line C++ tool that computes genome “mappability” (how unique each k-mer is) while allowing up to e mismatches, and it can export results in formats like WIG and BED/bedGraph. I installed it through Bioconda to avoid dealing with CMake/submodules and compiler issues. After installing, I could run the basic workflow: build an index from a FASTA file and then compute mappability, which writes text + wig + bedGraph outputs. Where I got stuck was reproducing the paper-level results (runtime/benchmark comparisons and the exact experimental setups). The repo is “not actively maintained,” and while it explains how to run the tool, it doesn’t provide a clean, one-command benchmark pipeline with pinned genome versions and settings that would let me recreate the paper’s tables/claims in a straightforward way. So the tool itself ran for me on test inputs, but I couldn’t fully replicate the paper’s reported benchmarking results.
Engineer: Haider Gilani 

Report #3
Github: https://github.com/HuffordLab/GenomeQC
Outcome: Failure
Process: I cloned the repo and followed the README. GenomeQC has a lot of dependencies, so instead of installing everything manually I used the Docker setup the repo provides.  After that, the tool ran and produced the expected QC outputs (summary metrics and plots), so the software itself worked. But this is mainly a general QC tool, not a step-by-step “reproduce these exact paper figures” pipeline. The repo doesn’t include one fixed dataset + exact commands/parameters tied to the paper’s reported results, so there wasn’t a clean way to recreate and quantitatively compare the paper’s figures.
Engineer: Haider Gilani

Report #4
Github: https://github.com/papaemmelab/Gao_NC_CH/tree/main
Outcome: Success
Process: I successfully replicated the findings of the Gao et al. Nature Communications paper using the authors’ publicly released analysis pipeline. After cloning the repository, I executed the full Jupyter notebook workflow, resolving minor environment issues related to R packages and directory setup. Once configured, the notebook ran smoothly and reproduced all major results, including genome‑wide mCA landscapes, mutation–mCA co‑occurrence patterns, composite genotype visualizations, and the survival and risk‑modeling analyses highlighted in the paper. The figures generated by my run matched the structure and conclusions of the publication, demonstrating that the authors’ analytical pipeline is fully reproducible using the processed CNV and mutation datasets provided in the repository. Despite the overall success, I encountered several reproducibility challenges typical of older R‑based bioinformatics pipelines. The notebook initially failed because the expected ../figures/ directory did not exist, causing repeated ggsave() write errors and Jupyter 404s until I manually created the folder. Several required R packages—such as sjPlot, survival, and cowplot—were missing and had to be installed manually. I also resolved multiple breaking changes introduced in newer versions of dplyr, including the deprecated left_join(..., fill = 0) syntax and mutate_at() functions, which required rewriting sections of the code using across() and replace_na(). Once these environment and syntax issues were fixed, the entire notebook executed cleanly and reproduced all figures and analyses from the paper.
Engineer: Haider Gilani 

Report #5
Github: https://github.com/OPUS-MaLab/opus_fold3 
Outcome: Failed.
Process: I cloned the opus_fold3 repository and read through the README to figure out how to run it. The repo is for protein folding and docking (it is basically a TensorFlow-based framework that generates 3D protein structures and lets you add differentiable constraints). The README also says the code was developed using Python + TensorFlow 2.4. My first issue was the environment. On my normal Python setup, installing TensorFlow 2.4 didn’t work because TensorFlow 2.4 is only built for older Python versions (PyPI lists Python 3.6, 3.7, and 3.8 for TF 2.4.0). I ended up making a separate conda environment with Python 3.8 so I could install tensorflow==2.4.0. After that, I still couldn’t get a clean run. The repo has scripts like fold_backbone.py, dock_backbone.py, etc., but the README doesn’t give a simple “run this command to reproduce results” section or a minimal working example. Instead, it mainly explains how to add your own constraint (it’s more like developer documentation than a reproducible pipeline). Without a clear entry point and without example inputs + expected outputs, I couldn’t confidently run the code end-to-end and verify that it worked. A second problem is that the paper provided is about clonal hematopoiesis and the interaction between gene mutations and mosaic chromosomal alterations. That topic doesn’t match what this repo is for (protein 3D structure prediction/folding). So even if OPUS-Fold3 ran perfectly, it would not reproduce the analyses or figures from that Nature Communications paper. It failed because the repo depends on an older TensorFlow/Python stack, doesn’t provide a clear reproducible “run pipeline,” and it doesn’t line up with the biology/problem in the provided paper.
Engineer: Haider Gilani

Report #6
Github: https://github.com/shefaliqamar/HIV-Epistatic
Outcome: Failure.
Process: I cloned the repo and found it’s basically just a few Jupyter notebooks with a very minimal README and no dataset bundled with the code. I set up a Python environment and started running the main notebook, but the whole project depends on getting the exact same input data the authors used from the Stanford HIV Drug Resistance Database. The paper says they analyzed 14,651 HIV-1 reverse transcriptase sequences labeled by treatment regimen, and their method produced 21 clusters. The problem is that it doesn’t clearly say which exact HIVDB export/query they used (and HIVDB has multiple different tools/ways to pull sequence data), so I tried a few reasonable options but couldn’t match the same dataset size/labels (hivdb.stanford.edu). Since my input data didn’t line up with theirs, the clustering results didn’t match what the paper reported, and I couldn’t claim a real reproduction.
Engineer: Haider Gilani
